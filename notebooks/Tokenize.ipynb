{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c6cb701",
   "metadata": {},
   "source": [
    "# Tokenize Data\n",
    "Take a dataset and tokenize the data so it can be passed into a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a819183a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src/') \n",
    "import Core as C\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "04ebaee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1a16f4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef0ccf5",
   "metadata": {},
   "source": [
    "Tokenize the testing Washington data (should take less than a second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "ba30e377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 134 items\n",
      "CPU times: user 395 ms, sys: 7.74 ms, total: 403 ms\n",
      "Wall time: 400 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with open('./Pickle_storage/test_data_WA.pkl', 'rb') as handle:\n",
    "    data = pickle.load(handle)\n",
    "\n",
    "data['numerical_tokens'] = []\n",
    "embed_map = None\n",
    "rm_list = []\n",
    "for idx in range(len(data['scores'])):\n",
    "    string = data['aligned_seqs'][idx]\n",
    "    if np.any(np.asarray([string.find(char) for char in {'D', 'H', 'K', 'M', 'V', 'R', 'N', 'X', 'S', 'W', 'Y'}]) != -1):\n",
    "        # We have a multistate character, flag it for removal from the dataset\n",
    "        rm_list.append(idx)\n",
    "        continue # Don't try to tokenize with multi-state characters\n",
    "    tokens = C.tokenizer.tokenize_seq(data['aligned_seqs'][idx], token_len=1)\n",
    "    numerical_tokens, embed_map, debed_map = C.tokenizer.onehot_tokens(tokens, embed_map=embed_map)\n",
    "    data['numerical_tokens'].append(numerical_tokens)\n",
    "rm_list.reverse()\n",
    "for idx in rm_list:\n",
    "    # Traverse the container largest to smallest\n",
    "    # Removing a larger indexed poisition won't impact smaller ones\n",
    "    for key in data.keys():\n",
    "        if key != 'numerical_tokens':\n",
    "            del data[key][idx] \n",
    "print('Removed', len(rm_list), 'items')\n",
    "rm_list = []\n",
    "\n",
    "with open('./Pickle_storage/test_data_tokenized_WA.pkl', 'wb') as handle:\n",
    "    pickle.dump(data, handle)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36ce58b",
   "metadata": {},
   "source": [
    "Tokenize the training data (non-WA). Should take 20 to 60 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "c2353377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 5579 items\n",
      "CPU times: user 24.5 s, sys: 1.24 s, total: 25.7 s\n",
      "Wall time: 25.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with open('./Pickle_storage/train_data.pkl', 'rb') as handle:\n",
    "    data = pickle.load(handle)\n",
    "\n",
    "\n",
    "data['numerical_tokens'] = []\n",
    "embed_map = None\n",
    "rm_list = []\n",
    "for idx in range(len(data['scores'])):\n",
    "    string = data['aligned_seqs'][idx]\n",
    "    if np.any(np.asarray([string.find(char) for char in {'D', 'H', 'K', 'M', 'V', 'R', 'N', 'X', 'S', 'W', 'Y'}]) != -1):\n",
    "        # We have a multistate character, flag it for removal from the dataset\n",
    "        rm_list.append(idx)\n",
    "        continue # Don't try to tokenize with multi-state characters\n",
    "    tokens = C.tokenizer.tokenize_seq(data['aligned_seqs'][idx], token_len=1)\n",
    "    numerical_tokens, embed_map, debed_map = C.tokenizer.onehot_tokens(tokens, embed_map=embed_map)\n",
    "    data['numerical_tokens'].append(numerical_tokens)\n",
    "rm_list.reverse()\n",
    "for idx in rm_list:\n",
    "    # Traverse the container largest to smallest\n",
    "    # Removing a larger indexed poisition won't impact smaller ones\n",
    "    for key in data.keys():\n",
    "        if key != 'numerical_tokens':\n",
    "            del data[key][idx] \n",
    "print('Removed', len(rm_list), 'items')\n",
    "rm_list = []\n",
    "\n",
    "with open('./Pickle_storage/train_data_tokenized.pkl', 'wb') as handle:\n",
    "    pickle.dump(data, handle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8273e2e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
